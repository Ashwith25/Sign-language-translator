{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.normalization import layer_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers.normalization import layer_normalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test image directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'D:\\gestures\\train'\n",
    "test_path = r'D:\\gestures\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 453 images belonging to 3 classes.\n",
      "Found 453 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 9s 164ms/step - loss: 3.9732 - accuracy: 0.8852 - val_loss: 0.1621 - val_accuracy: 0.9558\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 0.9647\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9603\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(3,activation =\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=100, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.973212480545044,\n",
       "  0.007903863675892353,\n",
       "  0.003748473944142461,\n",
       "  0.002554036444053054],\n",
       " 'accuracy': [0.8852097392082214, 1.0, 1.0, 1.0],\n",
       " 'val_loss': [0.1620788425207138,\n",
       "  0.12508046627044678,\n",
       "  0.14234574139118195,\n",
       "  0.14157262444496155],\n",
       " 'val_accuracy': [0.9558498859405518,\n",
       "  0.9646798968315125,\n",
       "  0.9624723792076111,\n",
       "  0.9602649211883545],\n",
       " 'lr': [0.001, 0.001, 0.001, 0.0005]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history2.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: signModelNew\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('signModelNew')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.00590427266433835; accuracy of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00590427266433835, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print('{0} of {1}; {2} of {3}%'\n",
    ".format(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1]*100))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.0026224867906421423; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(test_batches)\n",
    "model = keras.models.load_model(\"signModelNew\")\n",
    "# new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print('{0} of {1}; {2} of {3}%'\n",
    ".format(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 413,443\n",
      "Trainable params: 413,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels\n",
      "Three\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "Two\n",
      "One\n",
      "Three\n",
      "Two\n",
      "Actual labels\n",
      "Three\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "Two\n",
      "One\n",
      "Three\n",
      "Two\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "\n",
    "word_dict = {0:'One', 1:'Two', 2:'Three'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "print(\"Predicted labels\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)])\n",
    "    predicted_labels.append(word_dict[np.argmax(i)])\n",
    "    \n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)])\n",
    "    actual_labels.append(word_dict[np.argmax(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.9583333333333334\n",
      "Recall 0.8333333333333334\n",
      "f1_score 0.8666666666666666\n",
      "confusion_matrix: \n",
      "[[1 0 0]\n",
      " [0 1 1]\n",
      " [0 0 7]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Precision\", sk.metrics.precision_score(actual_labels, predicted_labels, average='macro'))\n",
    "print (\"Recall\", sk.metrics.recall_score(actual_labels, predicted_labels, average='macro'))\n",
    "print (\"f1_score\", sk.metrics.f1_score(actual_labels, predicted_labels, average='macro'))\n",
    "print (\"confusion_matrix: \")\n",
    "print (sk.metrics.confusion_matrix(actual_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbefc7fec24977a464f4b9c177a97fefd575b7fc826e54ce789a21be2d4f3465"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('signLanguage': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
